{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7e4ff7-358c-4f74-9cab-ae892ed0980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#We initalize a Layer of Size J, acting on input Vector X on of size K, with the associated Weigth Matrix, W of size J, K\n",
    "    #and the Bias Vector with size 1*J.\n",
    "class Layer: \n",
    "    def __init__(self, J, K):\n",
    "        np.random.seed(2)\n",
    "        #J: # of Neurons in layer ie the width of the Layer\n",
    "        self.J = J\n",
    "        #K: Size of Input Vetor\n",
    "        self.K = K\n",
    "        #W:weigth matrix of dimensions J x K , with  standard normal initializing\n",
    "        self.W = np.random.normal(0, 1, (J, K))\n",
    "        #B: Bias matrix of dimensions 1 x J, with standard normal initialzing  \n",
    "        self.B = np.random.normal(0, 1, J)\n",
    "        #Z: The last weighted input which the layer receives. Initalized at 0 \n",
    "        self.Z = 0\n",
    "        # A: The last activation A(Z). Initialized at 0 \n",
    "        self.A = 0\n",
    "        #g_l: Error of the Layer ie Gradient C with respect to weighted inputs Z in layer L, intialized at 0\n",
    "        self.g_l = 0\n",
    "        #sig_deriv: Derivative of sigmoind of forward pass:\n",
    "        self.sig_deriv = 0\n",
    "        #last_layer: True if output layer, else false. Initialized to be false\n",
    "        self.last_layer = False\n",
    "    \n",
    "    #Layer acting on Vector X, which is either the previous layer activations, or the first input vector.  \n",
    "    def forward_prop(self, X):\n",
    "        #Check Dimensionalty\n",
    "        if X.size != self.K:\n",
    "            return \"Error: Input Vector Dimensions do not match weight matrix\"\n",
    "        #Else proceed with forward propagation\n",
    "        else:\n",
    "            #Calculating weighted input Z\n",
    "            self.Z = np.dot(self.W, X) + self.B\n",
    "            #Passing Z trough Nonlinear Activation Function(, a Sigmoid in this case)\n",
    "            self.A = self.sigmoid()\n",
    "            #Store derivative of sigmoind\n",
    "            self.sig_deriv = self.deriv_sigmoid()\n",
    "            return self.A\n",
    "    # Returns the Error of the Last Layer for a training example.\n",
    "    # Let cost function take the form: C=0.5*(y_train-a_L)^2, where y_train is our desired output vector\n",
    "    def output_error(self, y_train):\n",
    "        sigmoid_derivative = self.deriv_sigmoid()\n",
    "        Gradient_C = -1 * (y_train - self.A)\n",
    "        #Hadamart Product:\n",
    "        Output_Error = sigmoid_derivative * Gradient_C\n",
    "        return Output_Error\n",
    "    #Nonlinear Activation Function\n",
    "    def sigmoid(self):\n",
    "        return 1.0 / (1.0 + np.exp(-1 * self.Z))\n",
    "    #Nonlinear derivative\n",
    "    def deriv_sigmoid(self):\n",
    "        return self.A * (1 - self.A)\n",
    "\n",
    "#NeuralNet: Class which defines how the individual layer object relate to each other. \n",
    "class NeuralNet:\n",
    "    #Pass in a List of layer sizes. The first index corresponds to input vector size, and the last index corresponds to the\n",
    "    #output vector size\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        # Iterate over pairs of adjacent layer sizes to create Layer instances\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            #Each layer has a width(number of neurons) and takes input of the size of the previous layer\n",
    "            layer = Layer(layer_sizes[i], layer_sizes[i - 1])\n",
    "            #last_layer: True if Outputlayer, else false\n",
    "            layer.last_layer = (i == len(layer_sizes) - 1)\n",
    "            #Appending individual layer to list\n",
    "            self.layers.append(layer)\n",
    "    #Forward pass for Input X:\n",
    "    def forward_pass(self, X):\n",
    "        input_Vector = X\n",
    "        for layer in self.layers:\n",
    "            input_Vector = layer.forward_prop(input_Vector)\n",
    "\n",
    "    def backward_pass(self, Y_train, X):\n",
    "        gradients_bias = []\n",
    "        gradients_weights = []\n",
    "        #Stores the output of the next layer to be resused for calculating the error. \n",
    "        next_layer_error = 0\n",
    "        #Reversing trough layers in reversed fashion, from the first index up to the last\n",
    "        \n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            #If Layer is the Output Layer:Calculate the output error. \n",
    "            if self.layers[i].last_layer:\n",
    "                Output_Error = self.layers[i].output_error(Y_train)\n",
    "                #Store Error of Layer for previous layer error calculations: \n",
    "                next_layer_error = Output_Error\n",
    "                self.layers[i].g_l = Output_Error\n",
    "                \n",
    "                #Calculate gradients of Bias terms \n",
    "                #grad_BL_C=Output_Error\n",
    "                \n",
    "                #Append Output Error to Gradient for Biases, since a perturbation to z is the same as perturbing b(linear addition). \n",
    "                gradients_bias.append(Output_Error)\n",
    "                \n",
    "                #Get previous layer activations to calculate the error: \n",
    "                #Previosu Layer if !First Layer:\n",
    "                prev_layer = self.layers[i - 1] if i != 0 else None\n",
    "                #Getting Activation or if the first Layer getting Input Vector X.\n",
    "                input_to_layer = prev_layer.A if prev_layer else X\n",
    "                #Calculating gradients for weigths.\n",
    "                grad_WL_C = np.outer(Output_Error, input_to_layer)\n",
    "                #Append Weight to return for gradient descent\n",
    "                gradients_weights.append(grad_WL_C)\n",
    "            #Consider hidden layers\n",
    "            else:\n",
    "                #Calculate Error as a function of the next Layer's error:\n",
    "                error_l = np.dot(self.layers[i + 1].W.T, self.layers[i + 1].g_l)\n",
    "                error_l = error_l * self.layers[i].sig_deriv\n",
    "                #Store error, for preceding layer\n",
    "                next_layer_error = error_l\n",
    "                self.layers[i].g_l = error_l\n",
    "                #Append Bias Gradient to return for gradient descent\n",
    "                gradients_bias.append(error_l)\n",
    "                #Calculate Gradient of Weigths\n",
    "                #If Hidden Layer do,\n",
    "                if i != 0:\n",
    "                    grad_Wl = np.outer(error_l, self.layers[i - 1].A)\n",
    "                #Else if First Layer do\n",
    "                else:\n",
    "                    grad_Wl = np.outer(error_l, X)\n",
    "                gradients_weights.append(grad_Wl)\n",
    "        #reverse gradient List such that the last entry corresponds to the last layers gradients\n",
    "        gradients_bias.reverse()\n",
    "        gradients_weights.reverse()\n",
    "        #Return for Gradient Descent:\n",
    "        return gradients_bias, gradients_weights\n",
    "    \n",
    "    #Function which creates a training Pass\n",
    "    #Input: Training Data:List of  Tuples of the X input Vector and Y_Train vector\n",
    "    #Batchsize\n",
    "    def training(self, Training_data, Batchsize, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            training_epoch(Training_data, Batchsize, learning_rate)\n",
    "        print(\"Training Complete\")\n",
    "        \n",
    "        \n",
    "    def training_epoch(self, Training_Data, Batchsize, learning_rate):\n",
    "        if Batchsize > len(Training_Data):\n",
    "            print(\"Batchsize larger than Training Data size. Adjusting Batchsize to Training Data size.\")\n",
    "            Batchsize = len(Training_Data)  # Adjust Batchsize to the size of Training_Data\n",
    "            \n",
    "            \n",
    "        #Create Batch by randomly selecting Batchsize # of traiing data.\n",
    "        Batch=random.sample(Training_Data, Batchsize)\n",
    "        \n",
    "        Gradients_Weights = []  \n",
    "        Gradients_Biases = []\n",
    "\n",
    "        \n",
    "        for member in Batch:\n",
    "            X_data= member[0]\n",
    "            Y_Train= member[1]\n",
    "            #Forward Pass: \n",
    "            self.forward_pass(X_data)\n",
    "            \n",
    "            #BackWard Pass to calculate gradients:\n",
    "            gradient_bias, gradient_weights = self.backward_pass(Y_Train, X_data)\n",
    "            \n",
    "            Gradients_Weights.append(gradient_weights)\n",
    "            Gradients_Biases.append(gradients_bias)\n",
    "        \n",
    "        # Get average of Gradient_Weights \n",
    "        # List to hold the sum of all gradient matrices\n",
    "        Grad_Weights_AVG = []\n",
    "        Grad_Bias_AVG = []\n",
    "\n",
    "        # Loop over every matrix (layer) in the gradient list\n",
    "        for matrix_index in range(len(Gradients_Weights[0])):\n",
    "            # Storing matrix index array from the first batch\n",
    "            Sum_Matrix = Gradients_Weights[0][matrix_index]\n",
    "            # Summing up over all matrix index arrays from each batch\n",
    "            for member_index in range(1, len(Gradients_Weights)):\n",
    "                Sum_Matrix += Gradients_Weights[member_index][matrix_index]\n",
    "            # Averaging the Gradient Matrix\n",
    "            Avg_Matrix = Sum_Matrix / len(Gradients_Weights)\n",
    "            Grad_Weights_AVG.append(Avg_Matrix)\n",
    "\n",
    "        # Loop over every matrix (layer) in the bias gradient list\n",
    "        for matrix_index in range(len(Gradients_Biases[0])):\n",
    "            # Storing matrix index array from the first batch\n",
    "            Sum_Matrix = Gradients_Biases[0][matrix_index]\n",
    "            # Summing up over all matrix index arrays from each batch\n",
    "            for member_index in range(1, len(Gradients_Biases)):\n",
    "                Sum_Matrix += Gradients_Biases[member_index][matrix_index]\n",
    "            # Averaging the Gradient\n",
    "            Avg_Matrix = Sum_Matrix / len(Gradients_Biases)\n",
    "            Grad_Bias_AVG.append(Avg_Matrix)\n",
    "\n",
    "        #Update the Weigths of each Layer:\n",
    "        for layer_index in range(len(self.layers)):\n",
    "            #Update Weigths:\n",
    "    \n",
    "            self.layers[layer_index].W=self.layers[layer_index].W-learning_rate*Grad_Weights_AVG[layer_index]\n",
    "        #Update Bias:\n",
    "            self.layers[layer_index].B=self.layers[layer_index].B-learning_rate*Grad_Bias_AVG[layer_index]     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cf3b2-943e-4999-90da-1cde1b1ce553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "530953e2-4a17-4125-a060-f3d9359ebf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (W):\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "Input Vector (X_Test):\n",
      "[1. 1. 1. 1.]\n",
      "Biases (B):\n",
      "[-0.87810789 -0.15643417  0.25657045 -0.98877905 -0.33882197]\n",
      "Manual Calculation of Z (Z_Test):\n",
      "[3.12189211 3.84356583 4.25657045 3.01122095 3.66117803]\n",
      "Manual Calculation of Activation (A_Test):\n",
      "[0.95778679 0.97903198 0.98602719 0.95307849 0.97494183]\n",
      "Layer Output Z (L1.Z):\n",
      "[3.12189211 3.84356583 4.25657045 3.01122095 3.66117803]\n",
      "Layer Output Activation (L1.A):\n",
      "[0.95778679 0.97903198 0.98602719 0.95307849 0.97494183]\n"
     ]
    }
   ],
   "source": [
    "#Test Case for Forward Prop\n",
    "# Initialize Layer\n",
    "L1 = Layer(5, 4)\n",
    "L1.W = np.ones((5, 4))\n",
    "X_Test = np.ones(4)\n",
    "\n",
    "# Manual Calculations\n",
    "Z_Test = np.dot(L1.W, X_Test) + L1.B\n",
    "A_Test = 1.0 / (1.0 + np.exp(-1 * Z_Test))\n",
    "\n",
    "# Print Initial Parameters\n",
    "print(\"Weights (W):\")\n",
    "print(L1.W)\n",
    "print(\"Input Vector (X_Test):\")\n",
    "print(X_Test)\n",
    "print(\"Biases (B):\")\n",
    "print(L1.B)\n",
    "\n",
    "# Print Manual Calculations\n",
    "print(\"Manual Calculation of Z (Z_Test):\")\n",
    "print(Z_Test)\n",
    "print(\"Manual Calculation of Activation (A_Test):\")\n",
    "print(A_Test)\n",
    "\n",
    "# Forward Propagation in Layer\n",
    "L1.forward_prop(X_Test)\n",
    "\n",
    "# Print Layer Output\n",
    "print(\"Layer Output Z (L1.Z):\")\n",
    "print(L1.Z)\n",
    "print(\"Layer Output Activation (L1.A):\")\n",
    "print(L1.A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c8173c2-a9d0-4e0c-80fc-d8b6d569e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Gradient: [-0.00025252]\n",
      "Analytical Gradient: -0.0002525163705311497\n",
      "Difference: [-2.51527043e-09]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the neural network with a larger architecture\n",
    "nn = NeuralNet([3, 5, 4, 7, 1])\n",
    "\n",
    "# Parameters for the test\n",
    "epsilon = 0.0001\n",
    "X_Test = np.random.rand(3)  # Random test input with 3 features\n",
    "Y_Train = np.array([1])     # Expected output\n",
    "\n",
    "# Forward and backward passes\n",
    "nn.forward_pass(X_Test)\n",
    "gradients_bias, gradients_weights = nn.backward_pass(Y_Train, X_Test)\n",
    "\n",
    "# Save the old weights and calculate the initial cost\n",
    "old_weights = nn.layers[0].W.copy()\n",
    "cost_nn = 0.5 * (Y_Train - nn.layers[-1].A) ** 2\n",
    "\n",
    "# Choose a specific weight in the first layer to perturb  and Adjust indices as needed\n",
    "\n",
    "weight_index = (0, 1)\n",
    "\n",
    "W_test = old_weights[weight_index]\n",
    "W_perturbed = W_test + epsilon\n",
    "old_weights[weight_index] = W_perturbed\n",
    "\n",
    "# Perform a forward pass with the perturbed weight\n",
    "nn.layers[0].W = old_weights\n",
    "nn.forward_pass(X_Test)\n",
    "\n",
    "# Calculate the new cost with the perturbed weight\n",
    "cost_new_nn = 0.5 * (Y_Train - nn.layers[-1].A) ** 2\n",
    "\n",
    "# Approximate the gradient numerically\n",
    "gradient_approx = (cost_new_nn - cost_nn) / epsilon\n",
    "\n",
    "# Extract the corresponding analytically computed gradient\n",
    "gradient_analytical = gradients_weights[0][weight_index]\n",
    "\n",
    "# Compare both gradients\n",
    "print(\"Numerical Gradient:\", gradient_approx)\n",
    "print(\"Analytical Gradient:\", gradient_analytical)\n",
    "print(\"Difference:\", gradient_approx - gradient_analytical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad9a8050-4ffd-4305-9e01-f693bbe8f2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Manual Calculation of Output Layer Error': array([-0.08872346]),\n",
       " 'Manual Calculation of Output Layer Bias gradient': array([-0.08872346]),\n",
       " 'Manual Calculation of Output Layer W gradient': array([[-0.08872346, -0.        ]]),\n",
       " 'Backpropagation Output Layer Error (g_l)': array([-0.08872346]),\n",
       " 'Calculated Gradients (Bias and Weights) from Backpropagation': {'Bias Gradient': array([-0.08872346]),\n",
       "  'Weights Gradient': array([[-0.08872346, -0.        ]])}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Case for BackProp Algo when the output layer is the second layer\n",
    "# Initialize a simple Neural Network: Input layer (2 neurons), Output layer (1 neuron) \n",
    "#Double check on piece of Paper\n",
    "nn = NeuralNet([2, 1])\n",
    "\n",
    "#print(nn.layers[0].B)\n",
    "\n",
    "# Set weights and biases to known values for simplicity\n",
    "nn.layers[0].W = np.array([[0.5, -0.5]]) # Weights for the first layer\n",
    "nn.layers[0].B = np.array([0.0])         # Biases for the first layer\n",
    "\n",
    "\n",
    "# Test input and expected output\n",
    "X_Test = np.array([1, 0])  # Test input\n",
    "Y_Train = np.array([1])    # Expected output\n",
    "\n",
    "# Perform forward pass\n",
    "nn.forward_pass(X_Test)\n",
    "\n",
    "# Manually calculate the output layer error (assuming sigmoid activation and mean squared error)\n",
    "last_layer = nn.layers[-1]\n",
    "Z_Test = last_layer.Z\n",
    "A_Test = last_layer.A\n",
    "Error_Test = (A_Test-Y_Train) * A_Test * (1 - A_Test)  # Derivative of MSE with respect to Z\n",
    "Gradient_B_Test= Error_Test\n",
    "\n",
    "Gradient_W_Test = np.outer(Error_Test, X_Test)\n",
    "\n",
    "\n",
    "\n",
    "# Perform backward pass\n",
    "gradients_bias, gradients_weights = nn.backward_pass(Y_Train, X_Test)\n",
    "\n",
    "# Printing results\n",
    "results = {\n",
    "    \"Manual Calculation of Output Layer Error\": Error_Test,\n",
    "    \"Manual Calculation of Output Layer Bias gradient\": Gradient_B_Test,\n",
    "    \"Manual Calculation of Output Layer W gradient\": Gradient_W_Test,\n",
    "    \"Backpropagation Output Layer Error (g_l)\": last_layer.g_l,\n",
    "    \"Calculated Gradients (Bias and Weights) from Backpropagation\": {\n",
    "        \"Bias Gradient\": gradients_bias[0],\n",
    "        \"Weights Gradient\": gradients_weights[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34c5f7-b505-4b2e-ada0-ebd83e9050df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
